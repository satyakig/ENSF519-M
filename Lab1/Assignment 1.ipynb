{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "## Satyaki Ghosh, 10077685\n",
    "\n",
    "**Assignment 1** - 100 marks\n",
    "\n",
    "**Due:** October 4th, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you were done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - The Zipf mystery (50 points)\n",
    "\n",
    "In this problem, we'd like to read the text from a book and perform some simple statistical analysis on the word counts. We have provided you with the actual text from [Lost On The Moon or, In Quest of the Field of Diamonds](https://www.goodreads.com/book/show/8636132-lost-on-the-moon-or-in-quest-of-the-field-of-diamonds) book in a file named 'the book.txt'. The file is cleaned up and only contains alphanumeric characters, i.e. no punctuation, quotation marks, etc.\n",
    "\n",
    "Read the file and break it down to its words. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_and_tokenize(file_name):\n",
    "    # Solution\n",
    "    return open(file_name, 'r').read().split()\n",
    "\n",
    "\n",
    "words = read_and_tokenize('the book.txt')\n",
    "words[1101:1111] # Expected: ['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sorted list of unique words in the book. Store the list in a variable called `V`. Also complete the `get_word_index` function below that gets a word and finds its index within `V`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "def find_uniques_and_sort(words):\n",
    "    unique_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "\n",
    "    unique_words.sort()\n",
    "    return unique_words\n",
    "\n",
    "\n",
    "V = find_uniques_and_sort(words)\n",
    "\n",
    "\n",
    "def get_word_index(word):\n",
    "    return V.index(word)\n",
    "\n",
    "get_word_index('about')  # Expected: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using no loops, and by only using `map` and `filter` built-in python functions traverse through the `V` (vocabulary) list above to find:\n",
    "\n",
    "* `long_words`: The list of words that have 10 letters or more \n",
    "* `no_vowels`: A list of all words but with vowels (aoeiu) removed. You can nest `map` and `filter` calls to iterate through the characters of the words.\n",
    "\n",
    "(5+5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "long_words = list(filter(lambda word: len(word) >= 10, V))\n",
    "no_vowels = list(map(lambda word: ''.join(list(filter(lambda ch: ch not in vowels, list(word)))), V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy array of size `|V|` that only contains 0s. Store it in a variable named `frequencies`. Use this array to count the number of times each word has appeared in the book. For example `frequencies[9]` should store how many times the word located in the index 9 of `V` (the sorted list) --which is the word \"about\"-- has been appreaed in the book (165 times). (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  1,  1, ..., 11,  1,  1]), 165)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Your solution\n",
    "frequencies = np.zeros(len(V), dtype=int)\n",
    "numpy_all_words = np.array(words)\n",
    "\n",
    "\n",
    "for i, word in enumerate(V):\n",
    "    frequencies[i] = len(np.where(numpy_all_words == word)[0])\n",
    "    \n",
    "    \n",
    "frequencies, frequencies[9] # Expected: array([ 1.,  1.,  1., ..., 11.,  1.,  1.]), 165.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word that appeared most frequently in the book. Find the word itself as well as the number of times it was repeated in the book. Use numpy functions, i.e. do not iterate over the `frequencies` array manually using a `for` loop. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\" is the most common word which has appeared 3237 times in the book.\n"
     ]
    }
   ],
   "source": [
    "# Your solution \n",
    "most_common_word = V[frequencies.argmax()]\n",
    "max_frequency = frequencies.max()\n",
    "\n",
    "print(f'\"{most_common_word}\" is the most common word which has appeared {max_frequency} times in the book.')\n",
    "# Expected: \"the\" is the most common word which has appeared 3237 times in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all frequency values by dividing them by the maximum frequency value (using vectorized operators). After this the most common word in the book should get a normalized frequency of `1` and all other words get some value \n",
    "between `1/MAX` and `1`. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00030893, 0.00030893, 0.00030893, ..., 0.00339821, 0.00030893,\n",
       "       0.00030893])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution\n",
    "normalized_frequencies = frequencies / max_frequency\n",
    "normalized_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check if the normalized frequencies have any corelation to their ranks. If such correlation exists, the Zipf's law states that it is linear in a log-log space. Take the logarithm of normalized frequencies (as y values) and create a numpy array of the same size containing the rank of each word (as x values). For example if the frequencies array is `[0.1, 1, 0.01, 0.0001]` the x and y values will be `X = [2, 1, 3, 4] Y=[-1, 0, -2, -4]`. \n",
    "\n",
    "You might want to sort the normalized frequencies first to make the task easier. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Your solution \n",
    "normalized_frequencies.sort()\n",
    "x = np.arange(normalized_frequencies.size, 0, -1, dtype=int)\n",
    "y = np.log10(normalized_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) on this data. The result is expected to be close to -1. Define appropriate functions for the the statistical calculations as neccessary. Additionally, you can use `pearsonr` function from `scipy` package to check if the calculated value is definitely correct. Though if you get a value close enough to -1 you can almost be sure that your implementation is correct and this step won't be necessary. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8631989095267898"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "def pcc(x, y):\n",
    "    x_means = x - x.mean()\n",
    "    y_means = y - y.mean()\n",
    "    \n",
    "    return (np.sum(x_means * y_means)) / np.sqrt(np.sum(np.square(x_means)) * np.sum(np.square(y_means)))\n",
    "\n",
    "pcc(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Log processing (50 points)\n",
    "\n",
    "In this part of the assignment we are going to use regular expressions to mine data out of some webserver log files. Although these problems can be solved without use of RegExes, but for this assignment you need to use them.\n",
    "\n",
    "A sample web server log file is provided along with this problem. In each line of the file one event is recorded. For simplicity all of the events in this file have the same format and are of the same type. Each event contains an ip address, date and time of the event, http method (`GET` or `POST`), a url, HTTP version, HTTP response code (usually 200), the response size in bytes, and the device's user agent which contains information about the device such as the brand and the operating system.\n",
    "\n",
    "Since these logs have such a well defined format regular expressions are the prefect tool for breaking them down into parts and perform different analysis on them.\n",
    "\n",
    "**Please make sure that when you are asked to write a function that _return_s something, you are _return_ing that value, not just _print_ing it**\n",
    "\n",
    "We start off with a random log line and write python functions that use regular expressions to break it off to pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "l = '5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"'\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the ip address part of the log line using regular expressions. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.106.145.204'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ip_address(l):\n",
    "    # Your solution here\n",
    "    return re.search('[^\\s]+', l).group(0)\n",
    "\n",
    "\n",
    "get_ip_address(l)  # Expected: '5.106.145.204'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the HTTP method, url, response code, and response size and returns a tuple. Use regular expressions. The http method is either `POST` or `GET` and the response code is always a 3 digit integer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_http_info(l):\n",
    "    # Your solution here\n",
    "    method, path, code, size = re.search('^.*\"(POST|GET|PUT|DELETE|PATCH)\\s([^\\s]+)\\s.*([\\d]{3})\\s([\\d]+)\\s.*$', l).groups()\n",
    "    return (method, path, int(code), int(size))\n",
    "\n",
    "\n",
    "get_http_info(l)  # Expected: ('POST', '/v1/crash-report/incident/report/', 200, 65)\n",
    "# Please note that the last two numbers are converted to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to break the date and time section apart and create a python datetime object based on that. Mind the time zone. convert the datetimes to MDT. Using `strptime` is a better solution in general, but for this assignment please stick to writing RegExes so you become more comfortable in writing and debugging them. (20 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 9, 4, 3, 21, 39, tzinfo=datetime.timezone(datetime.timedelta(-1, 64800)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from calendar import month_abbr\n",
    "\n",
    "MDT = timezone(timedelta(minutes=-6*60 + 0))\n",
    "\n",
    "def get_datetime(l):\n",
    "    # Your solution here\n",
    "    day, month, year, hour, mi, sec, tz_d, tz_h, tz_min = re.search('^.*\\[([\\d]{2})/(\\w+)/([\\d]{4}):([\\d]{2}):([\\d]{2}):([\\d]{2})\\s([\\+|\\-])([\\d]{2})([\\d]{2}).*$', l).groups()\n",
    "\n",
    "    tz_offset_min = int(tz_h) * 60 + int(tz_min)\n",
    "    if (tz_d == '-'):\n",
    "        tz_offset_min = tz_offset_min * -1\n",
    "    tz_offset = timezone(timedelta(minutes=tz_offset_min))\n",
    "\n",
    "    date = datetime(int(year), list(month_abbr).index(month), int(day), int(hour), int(mi), int(sec), tzinfo=tz_offset)\n",
    "    return date.astimezone(MDT)\n",
    "\n",
    "get_datetime(l)  # Expected: datetime.datetime(2019, 9, 4, 3, 21, 39, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file line by line and use the `get_datetime` and `get_http_info` functions above to calculate the used bandwidth of the server (the sum of all the response sizes) per hour. Use a `dict` or a `defaultdict`. (15 points)\n",
    "\n",
    "For example if there are 4 logs like:\n",
    "\n",
    "    Sep 4 14:20 .... 65bytes\n",
    "    Sep 4 14:35 .... 80bytes\n",
    "    Sep 4 15:01 .... 44bytes\n",
    "    Sep 5 18:20 .... 40bytes\n",
    "\n",
    "The result will be like:\n",
    "\n",
    "    Sep 4 14:00  145\n",
    "    Sep 4 15:00  44\n",
    "    Sep 5 18:00  40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bandwidth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Jul 20 2019 07:00</td>\n",
       "      <td>49130 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 20 2019 08:00</td>\n",
       "      <td>40469 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 20 2019 09:00</td>\n",
       "      <td>43556 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 20 2019 10:00</td>\n",
       "      <td>82526 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 20 2019 11:00</td>\n",
       "      <td>56328 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 26 2019 00:00</td>\n",
       "      <td>47222 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 26 2019 01:00</td>\n",
       "      <td>47935 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 26 2019 02:00</td>\n",
       "      <td>40544 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 26 2019 03:00</td>\n",
       "      <td>72866 bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jul 26 2019 04:00</td>\n",
       "      <td>11055 bytes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bandwidth\n",
       "date                          \n",
       "Jul 20 2019 07:00  49130 bytes\n",
       "Jul 20 2019 08:00  40469 bytes\n",
       "Jul 20 2019 09:00  43556 bytes\n",
       "Jul 20 2019 10:00  82526 bytes\n",
       "Jul 20 2019 11:00  56328 bytes\n",
       "...                        ...\n",
       "Jul 26 2019 00:00  47222 bytes\n",
       "Jul 26 2019 01:00  47935 bytes\n",
       "Jul 26 2019 02:00  40544 bytes\n",
       "Jul 26 2019 03:00  72866 bytes\n",
       "Jul 26 2019 04:00  11055 bytes\n",
       "\n",
       "[142 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution here\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_line(line):\n",
    "    method, path, code, size = get_http_info(line)\n",
    "    date = get_datetime(line)\n",
    "\n",
    "    return {\n",
    "        'bandwidth': size,\n",
    "        'date': date.strftime('%b %d %Y %H:00'),\n",
    "    }\n",
    "\n",
    "\n",
    "lines = open('log.txt', 'r').readlines()\n",
    "data = list(map(convert_line, lines))\n",
    "\n",
    "data_frame = pd.DataFrame(data)\n",
    "sum_bandwidth = data_frame.groupby('date').sum()\n",
    "sum_bandwidth['bandwidth'] = sum_bandwidth['bandwidth'].apply(lambda bandwidth: '{0} bytes'.format(bandwidth))\n",
    "sum_bandwidth\n",
    "\n",
    "\n",
    "# No specific format for the output is expected\n",
    "# However the data will be something like:\n",
    "#  2019, 7, 20 07:00    49130 bytes\n",
    "#  2019, 7, 20 08:00    40469 bytes\n",
    "#  2019, 7, 20 09:00    43556 bytes\n",
    "#  2019, 7, 20 10:00    82526 bytes .... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
